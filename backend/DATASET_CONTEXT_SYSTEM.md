# Dataset Context Storage System

## Overview
This document describes the async dataset context generation and storage system implemented in AutoDash.

## Components

### 1. Database Model (`backend/app/models.py`)

Added `Dataset` model to persist uploaded dataset metadata:

**Fields:**
- `id`: Primary key
- `user_id`: Foreign key to User
- `dataset_id`: Unique identifier (e.g., `upload_abc12345`, `sample_xyz78901`)
- `filename`: Original filename
- `row_count`, `column_count`: Dataset dimensions
- `file_size_bytes`: File size
- `context`: Rich textual description generated by DSPy (stored in Text column)
- `context_generated_at`: Timestamp when context was generated
- `columns_info`: JSON field storing column metadata, dtypes, and statistics
- `context_status`: Status of context generation ("pending", "generating", "completed", "failed")
- `created_at`, `updated_at`: Timestamps

**Relationship:**
- User has many Datasets
- Dataset belongs to User

### 2. Context Generator Service (`backend/app/services/context_generator.py`)

**Purpose:** Asynchronously generate rich dataset context using DSPy's `CreateDatasetContext` signature.

**Key Methods:**

#### `generate_context_async(dataset_id, df)`
- Runs context generation in thread pool to avoid blocking async loop
- Returns generated context string

#### `_generate_context_sync(dataset_id, df)`
- Updates database status to "generating"
- Prepares dataframe info (columns, dtypes, sample values, statistics)
- Calls DSPy's `ChainOfThought(CreateDatasetContext)` to generate context
- Stores result in database with status "completed"
- Handles errors and marks status as "failed" on exception

**Singleton Instance:** `context_generator` is a global singleton for reuse.

### 3. Updated Routes (`backend/app/routes/data.py`)

#### Modified Endpoints:

**`POST /api/data/upload`**
- Creates `Dataset` database record immediately after file processing
- Stores DataFrame in in-memory data store (for fast access)
- Triggers async context generation via `asyncio.create_task()`
- Returns with `context_status: "pending"`

**`POST /api/data/sample/load`**
- Same pattern as upload: DB record → in-memory store → async generation

#### New Endpoints:

**`GET /api/data/datasets/{dataset_id}/context`**
- Retrieve generated context for a specific dataset
- Returns:
  - `context`: The generated textual description
  - `context_status`: Current status
  - `context_generated_at`: Timestamp
  - `columns_info`: Detailed column metadata (JSON)
  - Basic dataset info (filename, dimensions, created_at)

**`GET /api/data/datasets`**
- List all datasets for the current user
- Returns array of dataset summaries with context_status
- Ordered by created_at descending (newest first)

## Data Flow

```
1. User uploads file
   ↓
2. File processed → DataFrame created
   ↓
3. Dataset record created in DB (status: "pending")
   ↓
4. DataFrame stored in memory for session use
   ↓
5. Async task launched for context generation
   ↓
6. Response returned immediately (non-blocking)
   ↓
7. [Background] DSPy generates context
   ↓
8. [Background] Context saved to DB (status: "completed")
   ↓
9. Frontend can poll GET /datasets/{id}/context
```

## Benefits

1. **Persistence**: Dataset metadata survives server restarts
2. **Non-blocking**: File upload returns immediately
3. **Rich Context**: DSPy generates detailed descriptions for better visualizations
4. **Status Tracking**: Frontend can show progress/completion
5. **User Isolation**: Each user's datasets are scoped by user_id
6. **Dual Storage**: DB for persistence + in-memory for speed

## Frontend Integration

### Polling Pattern (Recommended)
```typescript
// After upload
const { dataset_id } = await uploadFile(file);

// Poll for context completion
const pollInterval = setInterval(async () => {
  const { context_status, context } = await fetch(
    `/api/data/datasets/${dataset_id}/context`
  ).then(r => r.json());
  
  if (context_status === 'completed') {
    clearInterval(pollInterval);
    console.log('Context ready:', context);
  } else if (context_status === 'failed') {
    clearInterval(pollInterval);
    console.error('Context generation failed');
  }
}, 2000); // Poll every 2 seconds
```

### WebSocket Pattern (Future Enhancement)
Consider using WebSockets to push status updates to frontend instead of polling.

## Database Migration

After these changes, run the backend to auto-create the `datasets` table:
```bash
cd backend
python -m app.main
```

The table will be created automatically via SQLAlchemy's `Base.metadata.create_all()`.

## Testing

### Test Context Generation
```python
# In Python REPL or test script
import pandas as pd
from app.services.context_generator import context_generator

df = pd.read_csv('housing_sample.csv')
context = context_generator._generate_context_sync('test_123', df)
print(context)
```

### Test Endpoints
```bash
# Upload file
curl -X POST http://localhost:8000/api/data/upload \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -F "file=@data.csv"

# Check context status
curl http://localhost:8000/api/data/datasets/{dataset_id}/context \
  -H "Authorization: Bearer YOUR_TOKEN"

# List datasets
curl http://localhost:8000/api/data/datasets \
  -H "Authorization: Bearer YOUR_TOKEN"
```

## Future Enhancements

1. **Background Job Queue**: Use Celery or RQ for more robust background processing
2. **WebSocket Notifications**: Push context completion to frontend
3. **Context Caching**: Cache frequently accessed contexts in Redis
4. **File Storage**: Store uploaded files on disk/S3 for recovery after restart
5. **Context Versioning**: Track context regeneration history
6. **Batch Processing**: Generate contexts for multiple datasets in parallel

## Notes

- The in-memory data store is still used for fast DataFrame access during the session
- Dataset context is generated once and persisted
- If context generation fails, status is marked as "failed" and can be retried
- The DSPy `CreateDatasetContext` signature can be enhanced to generate more structured output

